{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is underfitting and overfitting? How do you want to avoid it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of finding patterns, if the model memorizes/ mimics patterns in the training data, it give sfalse predictions on unseen data. Curve derived from training model will then pass through all the data points. Accuracy on test data is low, this called overfitting, occurs due to high variance. \n",
    "Otherside, if the model performs well on test data but with low accuracy in regard to training data is called undefitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. what is cross validation? \n",
    "\n",
    "way of accessing accuracy of a model.. using resampling method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is ROC curve? Why is it useful over simply using the true positive and false positive rates?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROC means receiver operating characteristic curve. Trade off between TP and FP rates \n",
    "probability thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is AUC and what does it tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Area under the curve, method of evaluating classifier. Like to be close to 1 s much as possbile. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregating diff algorithms together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble methods - tarin many diffrenet models on the same dataset and aggregate the model predictions into one robust an dfault tolerant model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias variance trade off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "how do you reduce variance without increasing bias?\n",
    "averaging\n",
    "But, there begs the question: if we only have one training set where... \n",
    "    in this case we use BOOTSTRAPPING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramdom sample with replacement\n",
    "this method helps understand the bias and variance in a dataset. \n",
    "Basically, this is sampling with replacement, also called resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to decrease the variance of the model by generating additional trainig data\n",
    "we take repeated bootstrapped samples, run models, and average them together to reduce variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging does not typically affect bias. \n",
    "Bagging stands for boostrapped aggregating. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Is a method for converting rules of thumb and inclinations into rules for prediction\n",
    "boosting reduces bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "How? two steps:\n",
    "    a. produce a bunch of decently performing models\n",
    "    b. Sort based on a performance where those with good predictive results are given a higher weight in the final model. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is a type of ensemble method that runs multiple decision tree algorithms (often several hundred) where each of the trees gets a vote and the result/ prediction is determined by those votes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What is this also called?\n",
    "Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "# sudo pip install xgboost\n",
    "Extreme gradient boost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "#fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot a plt\n",
    "from sklearn.metrics import rox_curve, roc_auc_score\n",
    "\n",
    "#ROC curves are summaries of the trade-off between TP and FP\n",
    "#ROC for balanced dataset\n",
    "# precision recall curve summarises the trade-off between TP an dpositive predictive values\n",
    "#precision recall curves are best for imbalanced dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.Xlabel('False Positives Rate')\n",
    "plt.ylabel('True Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score, auc\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f1)\n",
    "\n",
    "auc = auc(recall, precision)\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recall, precision, marker='.')\n",
    "plt.Xlabel('Recall')\n",
    "plt.ylabel(\"Precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
